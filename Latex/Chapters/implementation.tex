% Chapter 3 - IMPLEMENTATION 

\chapter{Implementation} % Main chapter title

\label{chap:Implementation} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{Implementation details}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

All the implementation has been done in two dimensions on the unit square $(0,1)^2$. The goal of the implementation has been to investigate the virtues of the least-squares method. It is implemented with finite element and spectral basis functions and is compared to the results from standard Galerkin formulation. Finally the combined GLS-method described in chapter \ref{chap:newTheory} is also tested against the separate methods. The problems solved are simply expansions of the Poisson problem (All problems have been implemented with non-homogeneous Dirichlet boundary conditions). 
\begin{align}
	-\Delta u = f \text{  in  } \Omega
	\label{eq:PossionImplementation}
\end{align}
Then by adding a transport term you obtain the diffusion transport equation
\begin{align}
	-\mu \Delta u + \mathbf{b} \cdot \nabla u = f \text{ in } \Omega
	\label{eq:DiffTransImplementation}
\end{align}
This can be further expanded by adding a reaction term
\begin{align}
	-\mu \Delta u + \mathbf{b} \cdot \nabla u +\sigma u = f \text{ in } \Omega
	\label{eq:ReactionImplementation}
\end{align}
And as a final complication the vector field $\mathbf{b}$ can be made dependent on $u$ and hence give us the nonlinear diffusion transport reaction equation
\begin{align}
	-\mu \Delta u + \mathbf{b}(u) \cdot \nabla u +\sigma u = f \text{ in } \Omega
	\label{eq:ReactionImplementation}
\end{align}
%
All the equations listed above are transformed into a first order system of PDE's 
\begin{align}
	\mathcal{L}\mathbf{u} = \mathbf{f}.
	\label{eq:genFirstOrderFormulation}
\end{align}
The partial differential operator $\mathcal{L}$ for each problem is the one stated in chapter \ref{chap:theory}.
%
\section{The bilinear form obtained from least-squares}
 For the general problem\eref{eq:BVP} the functional $Q$ will take the form 
\begin{align}
	Q(u,v)=\int_{\Omega}(\mathcal{L}v)^T(\mathcal{L}u)d\Omega.
	\label{eq:functionalInt}
\end{align}
Implementing $Q$ requires two sets of basis functions $\{N_i\}$ that describes the search and solution space. In this project assignment the search and solution space will be described by the same set of basis functions which will depend on the method applied. $u$ is discretized as 
\begin{align}
	u_h = \sum_{I=0}^{K}a_IN_I.
	\label{eq:uDisc}
\end{align}
Where $K$ denotes the number of discretization points, which is the same as the number of basis functions. \colorbox{yellow}{Is this statement OK?} Since equation\eref{eq:varFormGen} requires equality for all test functions in the search space we simply solve it for each basis function. We are therefore left with a system of $K$ equations. Equation\eref{eq:functionalInt} can then be written for each test function as  
\begin{align}
	Q(u_h,N_I) &= \int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}u_h)d\Omega \\
	&= \int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}\sum_{J=1}^Ka_JN_J)d\Omega \\
	&= \sum_{J=1}^K\int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}a_JN_J)d\Omega \\
	&= \sum_{J=1}^K\int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}a_JN_J)d\Omega \\
	&= \sum_{J=1}^K\int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}N_J)d\Omega \;\cdot a_J.
	\label{eq:varFormDisc}
\end{align}
The total system of equation for all test functions can then be written as a matrix equation 
\begin{align}
	K^{LS}\mathbf{u} = F^{LS}.
	\label{eq:matrixEq}
\end{align}
Where $K^{LS}_{I,J}=\int_{\Omega}(\mathcal{L}N_I)^T(\mathcal{L}N_J)d\Omega$. \colorbox{yellow}{Should somehow show the importance of this formula}

Written explicitly for the \textbf{Poisson and diffusion transport equation} it will be a 3-by-3 matrix on the form 

$
K^{LS}_{I,J} = \int_{\Omega}
\begin{bmatrix}
	N_IN_J + \partial_x N_{I} \partial_xN_{J} & \partial_x N_{I}\partial_y N_{J} & N_IN_{J,x} \\ 	
	\partial_yN_{I}\partial_xN_{J} &N_IN_J + \partial_yN_{I}\partial_yN_{J} &  N_IN_{J,y} \\ 	
	N_{I,x}N_J & N_{I,y}N_J & N_{I,x}N_{J,x} + N_{I,y}N_{J,y} \\ 	
	\label{mat:basicPoisson}
\end{bmatrix}
d\Omega
$
where $\partial_x = \partial / \partial x $ for the Poisson problem and $\mu \partial / \partial x - b_1$ for the diffusion transport problem. Similarly $F_I$ will be given as 
\begin{align}
	F_I = \int_{\Omega}(\mathcal{L}N_I)^T\vec{f} d\Omega = 
	\int_{\Omega}
\begin{bmatrix}
	\partial_xN_I \\
	\partial_xN_I \\
	0
\end{bmatrix}
	f d\Omega
	\label{eq:rhsFunctional}
\end{align}

Notice that by doing the splitting of variables we obtain a system of equations three times as big as if we were to solve the equation directly. 

Note that the gradient of our solution must be restricted to the space $H^1(\text{div},\Omega) = \left\{  \mathbf{w}\in L^2(\Omega) | \nabla \cdot \mathbf{w} \in L^2(\Omega)\right\} $. In order to simplify the implementation I have in this project chosen my basis functions to a subspace such that $\mathbf{w}\in [H^1(\Omega)]^2$. In other words, the basis functions for the solution, and both components of the gradients are the same.  
\section{Least squares with finite element basis functions}
The same triangular grid has been chosen for the implementation of both regular Galerkin FEM and the least-squares FEM. By choosing 1st order hat functions each element consists of three non-zero basis functions. For the GFEM this leads to calculating a $3\times 3$ matrix for each element whereas for LSFEM a $9 \times 9$ matrix has to be calculated, although it has a relatively simple structure it still leads to additional computational costs and is a lot more tedious to implement.

\colorbox{yellow}{What can I say here\ldots not worth to show any derivations}
\section{Least squares with spectral basis functions}

The spectral implementation is done using Gauss Lobatto nodes and quadrature rule. The basis functions are then chosen as the lagrange functions based on the GL nodes. Notice that the discrete solution $\mathbf{u}_h$ consist of the discretization of both $u \text{ and } \mathbf{w} = - \nabla u$.\colorbox{yellow}{is it clear what system I am talking of here??} $\mathbf{u}_h$ can be structured block wise or node wise. By choosing a block wise representation the final system of equations surging from the Poisson problem can be written as 

$
\begin{bmatrix}
	K_{1,1} & K_{1,2} &	K_{1,3} \\ 	
	K_{2,1} & K_{2,2} & K_{2,3} \\ 	
	K_{3,1} & K_{3,2} & K_{3,3} \\ 	
\end{bmatrix}
\begin{bmatrix}
 u^h \\ 	
 w^h_1\\ 	
 w^h_2\\ 	
\end{bmatrix}
=
\begin{bmatrix}
 F_1 \\ 	
 F_2\\ 	
 0 \\ 	
\end{bmatrix}
$.

Where each block $K_{m,n}$ corresponds to  calculating element $m,n$ in the matrix\eref{mat:basicPoisson} for all combinations of $I,J$. In order to keep track of the indexing we use the expressions $I = i+jN$ and $J = k+lN$ where the helping indices denotes the position in $x,y$-coordinates. Let us take a closer look at element $K_{1,3}$ in order to achieve a more compact notation. 
\begin{align}
	(K_{1,3})_{I,J} &= \int_{\Omega} N_I N_{J,x} d\Omega \\
	&= \int_{\Omega} l_i(x)l_j(y)l'_k(x)l_l(y) d\Omega \\
	&= \sum_{\alpha}\sum_{\beta} w_{\alpha}w_{\beta}l_i(x_{\alpha})l_j(y_{\beta})l'_k(x_{\alpha})l_l(y_{\beta}).
	\label{eq:MatrixDerivation}
\end{align}
The sum is obtained by using Gauss Lobatto quadrature rule. Now notice that the lagrange polynomials $l_j(y_{\beta})$ are non-zero only when $\beta = j$, this implies that for the non-zero terms in the sum we have $\beta = j=l$, and $\alpha = i$. With these considerations the double sum above simplifies to  
\begin{align}
	(K_{1,3})_{I,J} &= w_{i}w_{j}l'_k(x_{i})
	\label{eq:MatrixDerivation2}
\end{align}
Remember that $I = i+jN$ and $J = k+lN$, with $N$ being the number of nodes in each spacial direction. This means that $K_{1,3}$ will consist of blocks where $i$ and $k$ goes from $1$ to $N$ while $j$ and $k$ are constant within each block. Since we require that $j=k$ we can immediately conclude that $K_{1,3}$ is nonzero only in the blocks along the diagonal. Notice that the factor $w_il'_k(x_i)$ is the same for each block, and it can be written in matrix form as $WL$. With this notation $W$ is the $n \times n$ diagonal matrix with the GLL-weights along the diagonal and $L$ is the matrix containing the derivatives of the lagrange polynomials $(L)_{i,j}= l_j'(x_i)$. Since each block $WL$ is multiplied with $w_j$ the whole matrix can simply be written as the Kronecker tensor product $(W\otimes WL)$. Similar reasoning can be made with all the other block matrices $K_{m,n}$ and for the Poisson problem we end up with a matrix on the form
%In order to implement this matrix it is convenient to write it in a compact form using the Kronecker tensor product. The components needed for this formulation is the $n \times n$ diagonal matrix $ W $ with the GLL-weigths along the diagonal and the $n \times n$ matrix $(L)_{i,j}= l_j'(x_i)$ where $l_j$ is the jth lagrange polynomial and $x_i$ is the ith node in either x or y direction. Note that the formulation is based on a grid of GLL-nodes in both x and y direction. 

%
$
K^{LS} = 
\begin{bmatrix}
	W \otimes (L^TWL+W) & WL \otimes L^TW 		 &	W \otimes WL  \\ 	
	L^TW \otimes WL     & (L^TWL+W) \otimes W  &	WL \otimes W  \\ 	
	W \otimes L^TW		  & L^TW \otimes W       &  L^TWL \otimes W +	W\otimes L^TWL  \\ 	
\end{bmatrix}
$

Note that without the reformulation of the PDE as a first order system and with regular Galerkin formulation the stiffness matrix will simply be 
$K_{3,3} = W \otimes L^TWL+ L^TWL \otimes W$.
\section{LS spectral method for the diffusion transport reaction equation}
The matrix corresponding to the discretized variational formulation can be divided into three parts due to its dependency on $\mu$,$\mathbf{b}$ and $\sigma$. When using galerkin formulation one can easily add extra terms, and the bilinear form expands as a superposition of the bilinear form from each term. In the least-squares setting this is not the case, each term added creates a bit more chaos, I have however tried to illustrate a way one can divide up the total matrix resulting from the diffusion transport equation
\begin{align}
	-\mu \Delta u + \mathbf{b} \cdot \nabla u +\sigma u = f \text{ in } \Omega
	\label{eq:ReactionImplementation}
\end{align}
\subsection{Laplacian part}
The part of the matrix which is closest related to the laplacian operator can be generated as 

$
A^{LS} = 
\begin{bmatrix}
	W \otimes (\mu L^TWL+W) & \mu WL \otimes L^TW 		 &	W \otimes WL  \\ 	
	\mu L^TW \otimes WL     & (\mu L^TWL+W) \otimes W  &	WL \otimes W  \\ 	
	W \otimes L^TW		  & L^TW \otimes W       &  L^TWL \otimes W +	W\otimes L^TWL  \\ 	
\end{bmatrix}
$

\subsection{Gradient part}
The gradient part will probvide us with a matrix that is dependent of $\mathbf{b}$. Let $B_1$ and $B_2$ be diagonal $n^2 \times n^2$ matrices with the values of the first and second component of $\mathbf{b}$ evaluated in each spacial node along the diagonal.  
\begin{align}
	G_{1,1} &= -\mu B_1 (W \otimes WL) -\mu( W \otimes L^TW) B_1 + B_1( W\otimes W) B_1 \\ 	
	G_{1,2} &= -\mu B_2 (W \otimes WL) -\mu (L^TW \otimes W) B_1 + B_1 (W\otimes W) B_2 \\
	G_{2,1} &= -\mu B_1 (WL \otimes W) -\mu (W \otimes L^TW) B_2 + B_2 (W\otimes W) B_1 \\
	G_{2,2} &= -\mu B_2 (WL \otimes W) -\mu (L^TW \otimes W) B_2 + B_2 (W\otimes W) B_2. 
	\label{eq:additionalMatrixDiffTrans}
\end{align}
%
Notice that $G_{2,1} &= G_{1,2}^T $, the least-squares formulation always provide a symmetric system of equations.
The full contribution from the gradient term given as a matrix is then given as  

$ G^{LS}=
\begin{bmatrix}
	G_{11} & G_{12} &0 \\
	G_{21} & G_{22} &0 \\
	0 & 0 & 0 
\end{bmatrix}.
$

\subsection{Reaction part}
The reactional term will consist of the following three $n^2 \times n^2$ sub matrices
\begin{align}
	R_{1,3} &= \sigma(\mu W\otimes L^T W - B_1 W\otimes W),\\
	R_{2,3} &= \sigma(\mu L^T W\otimes W - B_2 W\otimes W),\\
	R_{3,3} &= \sigma^2(W\otimes W).
	\label{ReactionalTerm}
\end{align}
Now, because of the symmetry guaranteed by the least-squares formulation $R_{3,1}=R_{1,3}^T$ and $R_{3,2}=R_{2,3}^T$. Hence the total attribution from the extra reactional term is 

$
R^{LS}=
\begin{bmatrix}
	0 & 0	 &	R_{1,3}   \\
	0 & 0	 &	R_{2,3}   \\
	R_{3,1} & R_{3,1} & R_{3,3} 
\end{bmatrix}.
$

\subsection{The loading function}
Adding the gradient and reactional term in our equation also affects the loading function. In a compact notation the discretized loading vector from the variational formulation can be written as
$
F^{LS}=
\begin{bmatrix}
	\mu(W \otimes L^T W)F_m - (W \otimes W)B_1F_m \\
	\mu(L^TW \otimes W)F_m - (W \otimes W)B_2F_m \\
	\sigma(W\otimes W)F_m & 
	\label{mat:FLS}
\end{bmatrix},
$
where $F_m$ is the vector with the loading function evaluated in each spacial node.
%

\subsection{Total matrix}
We can now define the total matrix for the diffusion transport reaction problem $K^{LS}$ as

\begin{align}
	K^{LS} = A^{LS} + G^{LS} + R^{LS}
	\label{eq:difftransMatrixSum}
\end{align}
%

Notice that most of the terms in the final matrix are cross terms, and does not surge solely from one of the terms in the equation. The way that the matrices have been divided into parts here are of now other than practical reasons and can be done in a different matter.  

\section{non-linear diffusion transport problem}
The stepwise algorithm to solve the nonlinear equation is described in chapter~\ref{chap:newTheory}. However there are several computational steps that needs to be taken care of, both with regular Galerkin and least squares. In both cases we obtain two matrices which we will name $A$ and $G$ and with superscript LS if they refer to the least squares formulation. In both cases only $G$ will depend on the numerical solution $u_h$. An important difference however is that in the LS setting the $F$ vector will depend on $u_h$ while in the straight forward Galerkin setting it will not. For regular Galerkin spectral approach we obtain
%
\begin{align}
	A \tilde{u} + AR_g  + G(\tilde{u}+R_g)(\tilde{u}+R_g) -F = 0
\end{align}
Notice that for each iteration the matrix $G(\tilde{u}+R_g)$ needs to be evaluated, the homogeneous boundary conditions on $\tilde{u}$ needs to be imposed and the Jacobian needs to be calculated. The Jacobian $\mathcal{F}$ will for this setting be given as
%
\begin{align}
	\mathcal{J}_{i,j} = A_{i,j} + G(\tilde{u}+R_g)_{i,j} +  (\tilde{u}+R_g)_i \; \frac{\partial}{\partial \tilde{u}_j} (G(\tilde{u}+R_g))_i.
\end{align}
%
With the LS formulation we obtain
\begin{align}
A^{LS}\tilde{u} + A^{LS}R_g  + G^{LS}(\tilde{u}+R_g)(\tilde{u}+R_g) -F^{LS}(\tilde{u}+R_g) = 0\\
A^{LS}\tilde{u} + A^{LS}R_g  + G^{LS}(\tilde{u}+R_g)\tilde{u} -F^{LS}(\tilde{u}+R_g) = 0
\end{align}
It is clear from this equation that the term surging from the loading function also needs to be handled when calculating the Jacobian. The lifting function $R_g$ does only have nonzero values in the third "block", hence it belongs to the kernel of the $G$-matrix.

\begin{align}
	\mathcal{J}_{i,j} = A_{i,j} + G(\tilde{u}+R_g)_{i,j} +  [\frac{\partial}{\partial \tilde{u}_j} (G(\tilde{u}+R_g))_{i,:}] \tilde{u} \; - \frac{\partial}{\partial \tilde{u}_j}F(\tilde{u}+R_g)_i.
\end{align}
%
Let us first consider the $F$-vector. The terms are given in equation\eref{mat:FLS} and it is clear that only the last terms in each block depends on $u$, notice that it does not depend on the components of the gradient $[ w_1 \; w_2]$. Hence the contribution to the total Jacobi matrix will only be in block $(1,3)$ and $(2,3)$. Further since $B_1$ and $B_2$ are both diagonal matrices where $B_{i,i}(u) = B_{i,i}(u_i)$ the Jacobian can be calculated efficiently by creating the matrices $dB_1$ and $dB_2$ which has the partial derivative of $B_1,B_2$ wrt. $u$ evaluated in each node.   



%The GLS-formulation is created simply by adding the variational formulation from the standard Galerkin and LS approach. By dividing the matrix surging from the bilinear functional into a linear and a non-linear part the system of equation can be written as 
%%
%\begin{align}
	%(A^{LS}+A)\mathbf{u_h} = -(G^{LS}(\mathbf{u_h})+G(\mathbf{u_h}))\mathbf{u_h} + f_h^{LS}(\mathbf{u_h})+f_h. \\
	%\mathcal{A} \mathbf{u_h} = \mathcal{F} (\mathbf{u_h})
	%\label{eq:nonlinMatrixFormulation}
%\end{align}
%%
%In order to solve this nonlinear system of equations I used newton iterations which can be divided into three steps 
%%
%\begin{enumerate}
	%\item $r^k = \mathcal{ F } (\mathbf{u_h}^k) - \mathcal{A}\mathbf{u_h}^k$   , Calculating the residual
	%\item $\hat{e}^k = \mathcal{J}_k^{-1}r^k $  , Calculating the error 
	%\item $\mathbf{u_h}^{k+1}=\mathbf{u_h}^k+\hat{e}^k$    , updating the solution
%\end{enumerate}
%%
%Where $\mathcal{J}_k$ is the jacobian matrix of $\mathcal{F}(\mathbf{u_h}^k)$

%\colorbox{blue}{mention the fact that LS formulation leads to a non-linear loading function}

%\colorbox{red}{How to deal with boundary conditions \ldots }
